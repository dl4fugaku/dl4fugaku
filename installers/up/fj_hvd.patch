diff --git a/horovod/torch/__init__.py b/horovod/torch/__init__.py
index 79c3e2f..fa3ad0d 100644
--- a/horovod/torch/__init__.py
+++ b/horovod/torch/__init__.py
@@ -125,8 +125,13 @@ class _DistributedOptimizer(torch.optim.Optimizer):
                 if p.requires_grad:
                     p.grad = p.data.new(p.size()).zero_()
                     self._requires_update.add(p)
-                    p_tmp = p.expand_as(p)
-                    grad_acc = p_tmp.grad_fn.next_functions[0][0]
+                    if p.is_mkldnn:
+                        p_tmp = p.to_dense().expand_as(p)
+                        grad_acc_tmp = p_tmp.grad_fn.next_functions[0][0]
+                        grad_acc = grad_acc_tmp.next_functions[0][0]
+                    else:
+                        p_tmp = p.expand_as(p)
+                        grad_acc = p_tmp.grad_fn.next_functions[0][0]
                     grad_acc.register_hook(self._make_hook(p))
                     self._grad_accs.append(grad_acc)
 
@@ -170,7 +175,8 @@ class _DistributedOptimizer(torch.optim.Optimizer):
         for p, (handle, _) in self._handles.items():
             output = synchronize(handle)
             self._allreduce_delay[p] = self.backward_passes_per_step
-            p.grad.set_(self._compression.decompress(output, ctx))
+            if not p.grad.is_mkldnn:
+                p.grad.set_(self._compression.decompress(output, ctx))
         self._handles.clear()
 
         self._synchronized = True
diff --git a/horovod/torch/mpi_ops.py b/horovod/torch/mpi_ops.py
index c2ee407..97b3704 100644
--- a/horovod/torch/mpi_ops.py
+++ b/horovod/torch/mpi_ops.py
@@ -88,8 +88,10 @@ def _check_function(function_factory, tensor):
 
 
 def _allreduce_function_factory(tensor):
-    return 'horovod_torch_allreduce_async_' + tensor.type().replace('.', '_')
-
+    if tensor.is_mkldnn:
+        return 'horovod_torch_allreduce_async_' + "torch.FloatTensor".replace('.', '_') # XXX
+    else:
+        return 'horovod_torch_allreduce_async_' + tensor.type().replace('.', '_')
 
 def _allreduce_async(tensor, output, name, op):
     if tensor.dtype == torch.float16 and not _fp16_supported:
@@ -334,8 +336,10 @@ def allgather(tensor, name=None):
 
 
 def _broadcast_function_factory(tensor):
-    return 'horovod_torch_broadcast_async_' + tensor.type().replace('.', '_')
-
+    if tensor.is_mkldnn:
+        return 'horovod_torch_broadcast_async_' + "torch.FloatTensor".replace('.', '_') # XXX
+    else:
+        return 'horovod_torch_broadcast_async_' + tensor.type().replace('.', '_')
 
 def _broadcast_async(tensor, output, root_rank, name):
     function = _check_function(_broadcast_function_factory, tensor)
diff --git a/horovod/torch/mpi_ops_v2.cc b/horovod/torch/mpi_ops_v2.cc
index d8c7a25..d2019d6 100644
--- a/horovod/torch/mpi_ops_v2.cc
+++ b/horovod/torch/mpi_ops_v2.cc
@@ -69,7 +69,7 @@ int DoAllreduce(::torch::Tensor tensor, ::torch::Tensor output, int divisor,
       [handle, divisor, output](const Status& status) mutable {
         // Will execute in the `device` context.
         if (divisor > 1) {
-          output.div_(divisor);
+          output.mul_(1./divisor);
         }
         handle_manager.MarkDone(handle, status);
       }, reduce_op);
